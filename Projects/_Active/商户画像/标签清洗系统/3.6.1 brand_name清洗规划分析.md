---
aliases:
date: 2025-09-20
---
---
收到。先把「怎么比、怎么判、怎么改」讲清楚，拍完板我再一次性给你 Python。

# 目标对齐

* 输入：Excel `/Users/didi/Downloads/panth/tag_ct/区域策略_品牌映射1.xlsx`，sheet「1」= 标准表；sheet「2」= 区域侧仅含 `enum_label` 的清单（其余列忽略）。
* 输出：在**不破坏 sheet1 既有结构**的前提下，—对—把 sheet2 的每个 `enum_label` 与 sheet1 的品牌做相似匹配；

  1. **完全一致/强匹配**：保留 sheet1，不新建，只在 sheet1 该品牌行打上“来自sheet2命中”的标记；若该别名在 `brand_aliases` 不存在，则追加别名；
  2. **弱匹配/不确定**：进入复核清单，不自动落库；
  3. **无匹配**：在 sheet1 里**新增一行品牌**（生成新 `enum_code`），并把 sheet2 的 `enum_label` 写入该行的 `brand_aliases`（含本名与常见缩写）。

---

# 匹配与归并逻辑（评分机制）

先做标准化，再做多通道相似度，取最高分及其通道，按阈值决策。

## 1) 文本标准化（**仅用于匹配，不改写原值**）

* 去除空格、全半角统一、中文标点统一、大小写统一；
* 去公司尾缀：`(股份)?有限责任公司|集团|能源|石油化工|石化|石油|加油站|（中国）|中国` 等**弱辨识词**（保留“中化/中海油/中石油/中石化/昆仑/壳牌/道达尔/BP/雪佛龙/加德士”等强辨识词）；
* 生成若干规范形态：原文、去尾缀版、拼音首字母（如 “中国海油”→ “ZGHY”）、常见缩写（规则库内置：`中国石化↔中石化`、`中国石油↔中石油`、`中国海洋石油↔中海油`、`壳牌↔SHELL`、`中国中化↔中化` 等）。

## 2) 相似度通道与权重

对 sheet2 的每个 `enum_label`（记作 q），与 sheet1 的每一行做以下通道的评分，取最高：

* T1 精确等值（原文或规范形态）= 1.00
* T2 命中现有 `brand_aliases` 的任一别名（规范形态后）= 0.99
* T3 包含/被包含（长度≥3 的中文子串或英文词）= 0.95
* T4 N-gram Jaccard（中文2-gram，英文按词）：得分 ∈\[0,1]
* T5 编辑距离（Levenshtein 归一化相似度）
* T6 拼音首字母相似（如 ZSH/ZSY 等）= 0.85 基线

综合分 = `max(各通道分)`（简单取最大，避免过拟合；需要可加通道权重）。

## 3) 决策阈值

* **≥ 0.92 → 强匹配**：认为与该品牌同一主体

  * 若 q 未在 `brand_aliases`，则把 q **追加为别名**；
  * 在该品牌行新增标记列 `s2_hit=1`、`s2_enum_label=q`、`sim_score`、`sim_channel`。
* **0.78 ≤ 分数 < 0.92 → 待复核**：写入 `review_list`（q、候选品牌、分数、通道、冲突证据），不自动入库。
* **< 0.78 → 无匹配**：在 sheet1 **新增品牌行**：

  * 生成新 `enum_code`（如 `BRAND_####` 递增）；
  * `enum_label` = q；
  * `brand_aliases` 至少包含：`q`、其常见缩写/规范形态（如“中海油”↔“中国海油/中国海洋石油”）。

---

# 空白字段的“可落地”补全规则（新增行或新增别名时）

> 你要的是“按 1 的表格拓展联想并补空”，这里给出**默认但可覆写**的安全规则集。

* `tag_code`：沿用 `brand_name`（与 sheet1 一致）。

* `spec_version`：沿用 sheet1 的版本（如 `1.0.0`）。

* `enum_code`：保留原值；新建时按 `BRAND_0001` 递增。

* `sort_order`：对新增品牌取 `max(sort_order)+1`；追加别名不改排序。

* `is_default`：仅“其他/other”行维持 `TRUE`；其余品牌为 `FALSE`。

* `brand_category`：基于关键词粗分（可后改）

  * 含“中石化/石化/中国石化”→ `石化`
  * 含“中石油/昆仑/中国石油”→ `石油`
  * 含“中海油/海油/海洋石油”→ `海油`
  * 含“壳牌/SHELL/道达尔/雪佛龙/BP/加德士”→ `外资`
  * 兜底：`其他`

* `keywords`：留空（避免与匹配规则冲突；后续可用 Top-N 2-gram 自动生）。

* `is_active`：1

* `match_method`：0（表示“规则+相似度”混合）

* `match_score_threshold`：1（别名白名单直连命中，不再二次打分）

* `white_list`：1（`brand_aliases` 走白名单精确/规范化命中）

* `black_list`：0

* `alias_norm_rule`：0（默认规范化策略编号；若后续需要可外置到 YAML）

* 新增**辅助列（仅用于这次对齐产出，可在落库前删除）**：

  * `s2_hit`（0/1）、`s2_enum_label`、`sim_score`、`sim_channel`、`decision`（HIT/ADD\_ALIAS/NEW/REVIEW）

---

# 核心输出：brand\_aliases & exclusion\_field 的自动构造

## 1) brand\_aliases（如何“追加/生成”）

* 若强匹配到已有品牌：`brand_aliases = 去重(原别名 ∪ {q 的原文、规范形态、常见缩写})`
* 若新建品牌：`brand_aliases = {q 原文, q 规范化, q 常见缩写/英文名（若识别得到）}`

> 常见缩写映射（内置规则库，后续你可补充 CSV）：
> 中国石化↔中石化；中国石油↔中石油/昆仑；中国海洋石油↔中国海油/中海油；壳牌↔SHELL；道达尔↔TOTAL；雪佛龙↔Chevron/加德士（区域语义谨慎处理）。

## 2) exclusion\_field（如何“智能补排除词”）

* 目标：减少“近邻品牌”误匹配。
* 方法：对全量品牌做**冲突词扫描**，取与本品牌 alias 在 2-gram 层面重叠度最高的 Top-K 竞争名（默认 K=3），把\*\*对方的“强辨识词/品牌全称”\*\*加入本行 `exclusion_field`。

  * 例：`中化石油` 的冲突 Top-K 可能含 `中海油/中石油/中石化/道达尔`，则本行 `exclusion_field` 自动并入这些对手的**强辨识词**（如“中海油, 中石油, 中石化, 道达尔”）。
* 去重、过滤过泛词（如“中国/集团/公司”等）。
* 你已有的人工排除维持优先级（人工 > 自动）。

---

# 变更策略与安全网

* **不覆盖**：任何自动更新只在副本表中完成（`sheet1_enriched`），原表不改动。
* **三清单导出**：

  1. `updates_alias.csv`（品牌行 + 追加的别名列表）；
  2. `new_brands.csv`（新建品牌的全量字段）；
  3. `review_list.csv`（待人工确认项：q、候选、分数、证据）。
* **可重复运行**：用哈希/集合去重，避免多次追加同一别名。
* **日志**：计数统计（命中/新增/复核），抽样 10 条供人工点验。

---

# 对你给的两个例子的小样结论（便于直觉校验）

* sheet2: `中国海油`

  * 若 sheet1 已有 `enum_label`=“中国海油”或 alias 含“中海油/中国海洋石油”→ 强匹配；在该品牌行 `s2_hit=1`，若 `brand_aliases` 未含“中国海油”，则**追加**。
  * 若 sheet1 不含任何海油品牌 → `new_brands.csv` 新增品牌“**中国海油**”，并自动生成别名 `{中国海油, 中海油, 中国海洋石油}`，`brand_category=海油`；`exclusion_field` 自动加入“中石油, 中石化, 壳牌”等 Top-K 冲突词。

---

# 你将拿到的交付物

* `sheet1_enriched.xlsx`：保留原列并新增辅助列，包含 alias 追加与新品牌行。
* `updates_alias.csv / new_brands.csv / review_list.csv`：三清单。
* 控制台统计摘要（命中率、复核率、Top 冲突词等）。

——
若以上逻辑 OK，你回我【代码】即可，我直接给出可运行的 Python（pandas + openpyxl），阈值、规则库、输出路径都写成可调参数。随后你只要改文件路径、按回车，就能跑通整套流程。

---
稳了，我把你的 a、b 两点并进到流程里，补齐“全局扩散 + 反山寨排除”的规则集。你拍板后我就一次性给 Python。

# A. 全局扩散：`brand_aliases_new` / `exclusion_field_new`

思路：对“合并后的品牌主名（canonical name）”做**行业词典 + 规则生成 + 全表冲突扫描**三步走，产出两列新字段（不覆盖旧列）。

1. Canonical 定锚

* 每个品牌锁一条“主称”作为锚点：如「中国石化」「中国石油」「中国海油」「中国中化」「壳牌」「道达尔」「雪佛龙」「BP」「加德士」「埃克森美孚」「中化弘润」「道达尔能源」「昆仑」等。
* 主称从 sheet1 的 `enum_label` 直接取；若为别名型条目则提升为主称。

2. 行业词典（内置，可再投喂增强）

* 中央系：

  * 中国石化↔中石化↔SINOPEC
  * 中国石油↔中石油↔CNPC↔昆仑
  * 中国海油↔中海油↔中国海洋石油↔CNOOC
  * 中国中化↔中化↔SINOCHEM
* 外资/合资常见：

  * 壳牌↔SHELL；道达尔↔TOTAL↔道达尔能源；BP↔英国石油；雪佛龙↔Chevron；加德士↔Caltex；埃克森美孚↔ExxonMobil；德西福格↔Tesco（如有地区误写）；嘉实多≠加德士（注意区分油品品牌 vs 零售品牌）。
* 民营高频（示例，按你库可继续扩）：宏昌、弘润、延长、广汇、金盾、金石、三友、吉泰、海科、元通、国通、海鹏、驰加、隆基、金龙、汇通、东明、恒力、裕隆、宝石、国能等（这些不与三桶油绑别名，只作各自的别名/排除候选）。

3. 规则生成（别名扩散，写入 `brand_aliases_new`）

* 规范化同义：去公司尾缀、地域前后缀（省市/集团/有限公司/能源/石化/石油/加油站等），保留强辨识词。
* 常见缩写：首字词（中国海洋石油→中国海油/中海油），英文名/缩写（SINOPEC/CNPC/CNOOC/SHELL/TOTAL/BP/Chevron/Caltex/ExxonMobil）。
* 组合形态：品牌+业态（如“××石化加油站”“××能源加油站”），品牌+区域（“××石化山东”“××石油北京”）——用于匹配，不强行写回别名文本中；写回别名仅保留“高信号形态”（短而稳）。
* 冲突去重：别名全集在全表 union 去重，长度<2 或过泛词丢弃（如“集团”“能源公司”）。

4. 全表冲突扫描（写入 `exclusion_field_new`）

* 为每个品牌，从其他品牌的**强辨识别名**中挑选“高混淆 Top-K”（默认 K=5）：

  * 2-gram/词粒度重叠高、编辑距离近、包含关系明显（如“中海油” vs “中国海油”）的**他牌名称**，加入本品牌 `exclusion_field_new`。
  * 三桶油之间互为排除：中石化↔中石油↔中海油。
  * 外资之间互排：壳牌/道达尔/BP/雪佛龙/加德士/美孚 等互相加入排除。
* 继承旧表：`exclusion_field_new = 去重(旧 exclusion_field ∪ 自动排除集)`。

# B. 反山寨/反污染的排除逻辑（关键）

目标：像你说的“新中石化”这类，**100%会命中主牌的词根**，但必须排除。

1. 山寨前后缀黑名单（命中即放入对应大牌的排除词集合）

* 前缀：新/准/仿/假/类/近似/亚/次/副/小/微/私/某/XX地区名+品牌（如“晋中石化”“鲁中石化”这类需人工白名单甄别）。
* 后缀：集团/实业/商贸/贸易/物流/运输/科技/化工/石化科技/能源科技/石油化工设备 等**非零售主体**。
* 组合：地域+品牌词根（“××中石化/××石化/××中石油/××海油”）若未在官方子公司白名单中，则加入**被排除**（可维护“官方子公司白名单”CSV，如“中石化销售股份有限公司××分公司”）。

2. 品牌词根库

* 词根：`中石化|中国石化|SINOPEC`；`中石油|中国石油|CNPC|昆仑`；`中海油|中国海油|CNOOC`；以及外资词根（SHELL/TOTAL/BP/Chevron/Caltex/ExxonMobil）。
* 规则：**任意“山寨前缀/后缀 + 词根”** → 归属该**词根品牌的 `exclusion_field_new`**，而不是其 `brand_aliases_new`。
* 例：`新中石化/中石化科技/河北中石化商贸` → 写入 **中国石化** 的 `exclusion_field_new`。

3. 合资公司处理

* 若“词根A + 词根B”共现（例：`中化-道达尔`），该条目不落入任一单一品牌 alias，进入**特殊规则**：

  * 若存在官方合资品牌名单（如“中化道达尔”确有零售合作主体），则作为**独立品牌**维护；
  * 否则默认加入双方的 `exclusion_field_new`（互斥），并标注 `decision=REVIEW_COOP`。

4. 强白名单与强黑名单

* **白名单优先**：你给的“官方主体/正式合资品牌”CSV，命中则允许作为该品牌别名；
* **黑名单兜底**：出现「××气站/液化气/柴油贸易/机修/商贸/运输」这类行业外词，默认进入排除，不加入任何品牌别名。

# 数据落地与字段产物

* 在合并后的表上新增两列：

  * `brand_aliases_new`：行业词典 + 规则扩散后的**高信号别名列表**（逗号分隔，去重）；
  * `exclusion_field_new`：自动冲突词 + 反山寨/合资互斥 + 旧排除并集。
* 保留你之前的 `brand_aliases/exclusion_field`；这两列是“增强版”，方便你对比与渐进替换。
* 另导出 `review_list.csv`：包含“疑似合资”“高混淆但不在白名单”“地域+品牌未识别为官方子公司”的条目。

# 是否需要你投喂完整版表？

不必等你再传我也能直接把代码写好（读取你给的路径、两张表结构、阈值/词典/黑白名单都做成可配置）；
但**如果你能一并给我**：

* ① 你现有的“官方子公司/官方合资品牌白名单”；
* ② 你们历史上确认过的“山寨/误写黑名单”；
  那命中率会更高、复核清单更短。没有也没关系，我会给一个**开箱可用的行业默认词典**，后续你补 CSV 即可无痛重跑。

# 下一步

你回“**代码**”，我就给出可运行的 Python：

* pandas + openpyxl；
* 行业词典/白黑名单可选 CSV；
* 自动生成 `brand_aliases_new/exclusion_field_new` + 三清单；
* 阈值与规则均可在顶部参数区改。

同时我会在代码里把“新中石化→中石化排除”的判定写成**可维护的正则与词根表**，确保 100% 命中且不误并。接下来还能加一个小评估报告（命中率、Top 冲突对、样例）。

---

给你一把可直接跑的脚本，**pandas + openpyxl**，不依赖外网包。按你给的路径默认读取 sheet「1」「2」，打分匹配、追加别名/新建品牌，并在合并后的表上新增两列：`brand_aliases_new`、`exclusion_field_new`（不覆盖老列）。同时导出三清单：`updates_alias.csv / new_brands.csv / review_list.csv`。

把下面整段保存为 `merge_brand_aliases.py`，在本机运行：

```bash
python3 merge_brand_aliases.py
```

```python
# -*- coding: utf-8 -*-
"""
区域清单(2) → 标准表(1) 合并脚本
- 读入:  /Users/didi/Downloads/panth/tag_ct/区域策略_品牌映射1.xlsx  (sheet: "1", "2")
- 输出:  同目录生成 enriched/ 下的结果文件
作者: 你们项目用
"""

import os
import re
import math
import unicodedata
from collections import defaultdict, Counter
from copy import deepcopy

import pandas as pd

# ========== 可调参数区 ==========
INPUT_XLSX = "/Users/didi/Downloads/panth/tag_ct/区域策略_品牌映射1.xlsx"
SHEET_STD = "1"         # 标准表
SHEET_LIST = "2"        # 区域清单，仅用 enum_label
OUT_DIR = os.path.join(os.path.dirname(INPUT_XLSX), "enriched")

# 匹配阈值（>=strong → 强匹配，>=review → 待复核）
SIM_THRESHOLD_STRONG = 0.92
SIM_THRESHOLD_REVIEW = 0.78

# 新 enum_code 前缀与数字宽度
ENUM_PREFIX = "BRAND_"
ENUM_PAD = 4

# 排除词 Top-K（按混淆度）
EXCL_TOPK = 5

# 行业词典（可继续补）
INDUSTRY_CANON_ALIASES = {
    "中国石化": ["中石化", "SINOPEC", "中国石化销售", "中国石化集团"],
    "中国石油": ["中石油", "CNPC", "昆仑", "中国石油天然气"],
    "中国海油": ["中海油", "中国海洋石油", "CNOOC"],
    "中国中化": ["中化", "SINOCHEM", "中化弘润", "弘润"],
    "壳牌": ["SHELL", "壳牌石油"],
    "道达尔": ["TOTAL", "道达尔能源", "TotalEnergies"],
    "雪佛龙": ["Chevron", "雪佛龙石油"],
    "BP": ["英国石油", "British Petroleum"],
    "加德士": ["Caltex"],
    "埃克森美孚": ["ExxonMobil", "美孚"],
}

# 三桶油/外资互排的“强辨识名”（用于冲突扫描时作为强信号）
STRONG_TOKENS = {
    "中国石化": ["中国石化", "中石化", "SINOPEC"],
    "中国石油": ["中国石油", "中石油", "CNPC", "昆仑"],
    "中国海油": ["中国海油", "中海油", "CNOOC", "中国海洋石油"],
    "壳牌": ["壳牌", "SHELL"],
    "道达尔": ["道达尔", "TOTAL", "TotalEnergies"],
    "雪佛龙": ["雪佛龙", "Chevron"],
    "BP": ["BP", "英国石油"],
    "加德士": ["加德士", "Caltex"],
    "埃克森美孚": ["埃克森美孚", "ExxonMobil", "美孚"],
    "中国中化": ["中国中化", "中化", "SINOCHEM", "中化弘润"],
}

# 山寨/污染前后缀
FAKE_PREFIX = ["新", "准", "仿", "假", "类", "亚", "次", "副", "小", "微"]
FAKE_SUFFIX = ["集团", "实业", "商贸", "贸易", "物流", "运输", "科技", "化工", "设备", "机修", "配件", "材料"]

# 词根库（用于“新中石化”等反山寨）
BRAND_ROOTS = {
    "中国石化": ["中国石化", "中石化", "SINOPEC"],
    "中国石油": ["中国石油", "中石油", "CNPC", "昆仑"],
    "中国海油": ["中国海油", "中海油", "中国海洋石油", "CNOOC"],
    "中国中化": ["中国中化", "中化", "SINOCHEM", "弘润", "中化弘润"],
    "壳牌": ["壳牌", "SHELL"],
    "道达尔": ["道达尔", "TOTAL", "TotalEnergies"],
    "雪佛龙": ["雪佛龙", "Chevron"],
    "BP": ["BP", "英国石油"],
    "加德士": ["加德士", "Caltex"],
    "埃克森美孚": ["埃克森美孚", "ExxonMobil", "美孚"],
}

# 官方白名单/黑名单（如有文件可改为读 CSV，这里给空集）
OFFICIAL_WHITELIST = set()  # e.g., {"中化道达尔", "中国石化销售股份有限公司××分公司"}
HARDCODED_BLACK = set()     # e.g., {"某某气站", ...}

# ========== 工具函数 ==========

def ensure_dir(p):
    if not os.path.exists(p):
        os.makedirs(p, exist_ok=True)

def z2h(s: str) -> str:
    return unicodedata.normalize("NFKC", s)

COMPANY_TAIL = re.compile(r"(（中国）|中国)?(股份)?有限(责任)?公司|集团|公司|石油化工|石油天然气|石油|石化|能源|加油站|有限公司|分公司|总公司")
SPACE_PUNC = re.compile(r"[\s·•\-\_（）\(\)【】\[\]<>《》“”\"\'，,。!！?？:：;；/\\]+")

def normalize_for_match(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = z2h(s.strip().lower())
    s = SPACE_PUNC.sub("", s)
    s = COMPANY_TAIL.sub("", s)
    return s

def split_alias(text) -> list:
    if not isinstance(text, str) or not text.strip():
        return []
    parts = re.split(r"[,\|，、；;／/ ]+", z2h(text).strip())
    return [p for p in map(str.strip, parts) if p]

def join_list_unique(items) -> str:
    seen, out = set(), []
    for x in items:
        if not x: 
            continue
        if x not in seen:
            out.append(x)
            seen.add(x)
    return ",".join(out)

def ngram_tokens(s: str, n=2):
    s = normalize_for_match(s)
    if not s:
        return set()
    if len(s) <= n:
        return {s}
    return {s[i:i+n] for i in range(len(s)-n+1)}

def jaccard(a: str, b: str, n=2) -> float:
    A, B = ngram_tokens(a, n), ngram_tokens(b, n)
    if not A or not B:
        return 0.0
    return len(A & B) / len(A | B)

def edit_distance(a: str, b: str) -> int:
    a, b = normalize_for_match(a), normalize_for_match(b)
    la, lb = len(a), len(b)
    if la == 0: return lb
    if lb == 0: return la
    dp = list(range(lb+1))
    for i in range(1, la+1):
        prev, dp[0] = dp[0], i
        for j in range(1, lb+1):
            cur = dp[j]
            cost = 0 if a[i-1] == b[j-1] else 1
            dp[j] = min(dp[j] + 1,      # deletion
                        dp[j-1] + 1,    # insertion
                        prev + cost)    # substitution
            prev = cur
    return dp[lb]

def ed_sim(a: str, b: str) -> float:
    # 归一化编辑距离到[0,1]
    a_n, b_n = normalize_for_match(a), normalize_for_match(b)
    m = max(len(a_n), len(b_n))
    if m == 0:
        return 0.0
    return 1.0 - (edit_distance(a, b) / m)

def token_contains(a: str, b: str) -> bool:
    # 是否存在长度>=3 的包含关系（中文/英文都考虑）
    an, bn = normalize_for_match(a), normalize_for_match(b)
    if len(an) >= 3 and bn in an:
        return True
    if len(bn) >= 3 and an in bn:
        return True
    return False

def best_similarity(q: str, candidates: list) -> tuple:
    """
    返回 (best_index, best_score, channel)
    channel: T1=exact, T2=alias, T3=contain, T4=jaccard, T5=edit
    """
    q_norm = normalize_for_match(q)
    best = (-1, 0.0, "NONE")
    for idx, cand in enumerate(candidates):
        cand_norm = normalize_for_match(cand)
        # T1: exact
        if q_norm and q_norm == cand_norm:
            return (idx, 1.0, "T1")
        # T3: contain
        if token_contains(q, cand):
            sc = 0.95
            if sc > best[1]:
                best = (idx, sc, "T3")
        # T4: jaccard
        sc4 = jaccard(q, cand, 2)
        if sc4 > best[1]:
            best = (idx, sc4, "T4")
        # T5: edit
        sc5 = ed_sim(q, cand)
        if sc5 > best[1]:
            best = (idx, sc5, "T5")
    return best

def next_enum_code(existing_codes):
    mx = 0
    for c in existing_codes:
        if isinstance(c, str) and c.startswith(ENUM_PREFIX):
            try:
                mx = max(mx, int(c.replace(ENUM_PREFIX, "")))
            except:
                pass
    return lambda k: f"{ENUM_PREFIX}{str(k).zfill(ENUM_PAD)}"

def expand_aliases(canon_name: str) -> list:
    base = set()
    base.add(canon_name)
    base |= set(INDUSTRY_CANON_ALIASES.get(canon_name, []))
    # 常见形态：去尾缀版本
    base |= {z2h(canon_name)}
    # 去过泛词
    cleaned = []
    for a in base:
        an = re.sub(SPACE_PUNC, "", a.strip())
        an = COMPANY_TAIL.sub("", an)
        if len(an) >= 2:
            cleaned.append(an)
    return sorted(set(cleaned), key=lambda x: (len(x), x))

def detect_fake_to_exclude(canon_name: str) -> list:
    roots = []
    for root, variants in BRAND_ROOTS.items():
        if root == canon_name:
            roots = variants
            break
    out = set()
    for r in roots:
        r_n = normalize_for_match(r)
        # 前缀+词根
        for pre in FAKE_PREFIX:
            out.add(pre + r)
        # 词根+后缀
        for suf in FAKE_SUFFIX:
            out.add(r + suf)
        # 一些常见组合（地域+词根），仅模板提示，不做地域枚举
        out.add("某某" + r)  # 示例占位，提醒审核
        # 规范化形式也加入
        out.add("新" + r_n)
    # 去重弱词
    result = sorted(set([z2h(x) for x in out if len(normalize_for_match(x)) >= 3]))
    return result

def top_confusions(all_canon_names: list, self_name: str, topk=EXCL_TOPK) -> list:
    conf = []
    for other in all_canon_names:
        if other == self_name:
            continue
        # 用强信号别名比较
        a = " ".join(STRONG_TOKENS.get(self_name, [self_name]))
        b = " ".join(STRONG_TOKENS.get(other, [other]))
        score = max(jaccard(a, b, 2), ed_sim(a, b))
        # 也考虑包含关系加权
        if token_contains(a, b):
            score = max(score, 0.9)
        conf.append((other, score))
    conf.sort(key=lambda x: x[1], reverse=True)
    return [x[0] for x in conf[:topk]]

# ========== 主流程 ==========

def main():
    ensure_dir(OUT_DIR)
    xls = pd.ExcelFile(INPUT_XLSX)
    df1 = pd.read_excel(xls, SHEET_STD)
    df2 = pd.read_excel(xls, SHEET_LIST)

    # 标准化列名（防止大小写/空格差异）
    df1.columns = [str(c).strip() for c in df1.columns]
    df2.columns = [str(c).strip() for c in df2.columns]

    # 关键列兜底
    for col in ["tag_code","spec_version","enum_code","enum_label","sort_order","is_default",
                "brand_aliases","exclusion_field","brand_category","keywords","is_active",
                "match_method","match_score_threshold","white_list","black_list","alias_norm_rule"]:
        if col not in df1.columns:
            df1[col] = None

    # —— 备份原表，不覆盖
    enriched = df1.copy()

    # 建立“候选对照”集合：以 enum_label 为主名；也收集 brand_aliases 便于 T2
    canon_names = list(enriched["enum_label"].astype(str).fillna(""))
    enum_codes = list(enriched["enum_code"].astype(str).fillna(""))

    # 为 T2 构建 alias 到主名的映射
    alias2canon = defaultdict(set)
    for _, row in enriched.iterrows():
        canon = str(row.get("enum_label", "")).strip()
        for a in split_alias(row.get("brand_aliases", "")):
            alias2canon[normalize_for_match(a)].add(canon)
        # 主名本身也当作 alias
        alias2canon[normalize_for_match(canon)].add(canon)

    # 处理 sheet2 枚举值
    s2_labels = df2["enum_label"].dropna().astype(str).map(str.strip).tolist()

    # 生成新 enum_code 序号器
    seq = next_enum_code(enum_codes)
    next_id = 1 + max([int(c.replace(ENUM_PREFIX, "")) for c in enum_codes if isinstance(c, str) and c.startswith(ENUM_PREFIX)] + [0])

    updates_alias_rows = []   # 对已有品牌追加的别名
    new_brand_rows = []       # 新增品牌的全量行
    review_rows = []          # 待复核

    # 先准备候选名列表（用于 best_similarity：只对 canon_names）
    candidates = canon_names

    # s2 命中标记列
    enriched["s2_hit"] = 0
    enriched["s2_enum_label"] = ""
    enriched["sim_score"] = 0.0
    enriched["sim_channel"] = ""
    enriched["decision"] = ""

    # —— 遍历 sheet2 的每个 enum_label
    for q in s2_labels:
        # T2: 是否直接命中任何 alias
        alias_norm = normalize_for_match(q)
        t2_hits = list(alias2canon.get(alias_norm, []))

        if t2_hits:
            # 命中多个就取“最佳相似”的那个
            idx, sc, ch = best_similarity(q, t2_hits)
            hit_canon = t2_hits[idx]
            # 记录
            pos = enriched.index[enriched["enum_label"] == hit_canon]
            if len(pos) > 0:
                i = pos[0]
                enriched.at[i, "s2_hit"] = 1
                enriched.at[i, "s2_enum_label"] = q
                enriched.at[i, "sim_score"] = 0.99
                enriched.at[i, "sim_channel"] = "T2"
                enriched.at[i, "decision"] = "ADD_ALIAS"
                # 追加别名（若不存在）
                old_alias = split_alias(enriched.at[i, "brand_aliases"])
                if q not in old_alias:
                    updates_alias_rows.append({
                        "enum_code": enriched.at[i, "enum_code"],
                        "enum_label": hit_canon,
                        "new_alias": q
                    })
                continue

        # 非 alias 直命中，做综合相似
        j, score, channel = best_similarity(q, candidates)
        if j >= 0 and score >= SIM_THRESHOLD_STRONG:
            hit_canon = candidates[j]
            pos = enriched.index[enriched["enum_label"] == hit_canon]
            if len(pos) > 0:
                i = pos[0]
                enriched.at[i, "s2_hit"] = 1
                enriched.at[i, "s2_enum_label"] = q
                enriched.at[i, "sim_score"] = score
                enriched.at[i, "sim_channel"] = channel
                enriched.at[i, "decision"] = "ADD_ALIAS"
                old_alias = split_alias(enriched.at[i, "brand_aliases"])
                if q not in old_alias:
                    updates_alias_rows.append({
                        "enum_code": enriched.at[i, "enum_code"],
                        "enum_label": hit_canon,
                        "new_alias": q
                    })
            continue

        if j >= 0 and score >= SIM_THRESHOLD_REVIEW:
            # 待复核
            review_rows.append({
                "s2_enum_label": q,
                "candidate_enum_label": candidates[j],
                "sim_score": round(score, 4),
                "sim_channel": channel,
            })
            continue

        # —— 新建品牌
        new_code = f"{ENUM_PREFIX}{str(next_id).zfill(ENUM_PAD)}"
        next_id += 1

        # 基于行业词典扩散别名
        canon = q  # 新品牌主名取 q
        alias_new = expand_aliases(canon)
        # brand_category 粗分
        brand_cat = "其他"
        if any(x in canon for x in ["中石化", "石化"]):
            brand_cat = "石化"
        elif any(x in canon for x in ["中石油", "昆仑", "中国石油"]):
            brand_cat = "石油"
        elif any(x in canon for x in ["中海油", "海油", "海洋石油"]):
            brand_cat = "海油"
        elif any(x in canon for x in ["壳牌","SHELL","道达尔","TOTAL","BP","雪佛龙","Chevron","加德士","Caltex","美孚","ExxonMobil"]):
            brand_cat = "外资"

        # 填全字段（按你的默认规则）
        row = {
            "tag_code": "brand_name",
            "spec_version": enriched["spec_version"].mode().iat[0] if not enriched["spec_version"].isna().all() else "1.0.0",
            "enum_code": new_code,
            "enum_label": canon,
            "sort_order": (enriched["sort_order"].max() or 0) + 1,
            "is_default": False,
            "brand_aliases": join_list_unique(alias_new),
            "exclusion_field": "",   # 先空，后面统一生成 _new
            "brand_category": brand_cat,
            "keywords": "",
            "is_active": 1,
            "match_method": 0,
            "match_score_threshold": 1,
            "white_list": 1,
            "black_list": 0,
            "alias_norm_rule": 0,
            "s2_hit": 1,
            "s2_enum_label": q,
            "sim_score": 0.0,
            "sim_channel": "NEW",
            "decision": "NEW",
        }
        new_brand_rows.append(row)

        # 更新候选集/映射
        candidates.append(canon)
        alias2canon[normalize_for_match(canon)].add(canon)
        for a in alias_new:
            alias2canon[normalize_for_match(a)].add(canon)

    # 把新建品牌并入 enriched 副本
    if new_brand_rows:
        enriched = pd.concat([enriched, pd.DataFrame(new_brand_rows)], ignore_index=True)

    # —— 基于“合并后的主名”生成 brand_aliases_new / exclusion_field_new
    # 1) 计算每个主名的扩散别名候选
    canon_to_alias_new = {}
    all_canon = enriched["enum_label"].astype(str).tolist()

    for canon in all_canon:
        # 起点：原别名 + 行业词典扩散
        row = enriched[enriched["enum_label"] == canon].iloc[0]
        old_alias = set(split_alias(row.get("brand_aliases", "")))
        expanded = set(expand_aliases(canon)) | old_alias

        # 加一点安全的缩写同义（如果存在）
        for root, vs in BRAND_ROOTS.items():
            if canon == root:
                expanded |= set(vs)

        canon_to_alias_new[canon] = sorted(expanded, key=lambda x: (len(x), x))

    # 2) 反山寨/污染排除 + Top-K 冲突
    canon_to_excl_new = {}
    for canon in all_canon:
        excl = set(split_alias(enriched.loc[enriched["enum_label"] == canon, "exclusion_field"].fillna("").astype(str).iloc[0]))

        # 反山寨
        excl |= set(detect_fake_to_exclude(canon))

        # Top-K 冲突品牌的强信号词汇
        conf_others = top_confusions(all_canon, canon, EXCL_TOPK)
        for other in conf_others:
            excl |= set(STRONG_TOKENS.get(other, [other]))

        # 外资/三桶油互排（强约束）
        if canon in STRONG_TOKENS:
            for other in STRONG_TOKENS:
                if other != canon:
                    excl |= set(STRONG_TOKENS[other])

        # 黑名单兜底
        excl |= HARDCODED_BLACK

        # 去弱/去重
        excl_clean = []
        seen = set()
        for x in excl:
            x = z2h(str(x).strip())
            if not x or len(normalize_for_match(x)) < 2:
                continue
            if x not in seen:
                excl_clean.append(x); seen.add(x)

        canon_to_excl_new[canon] = excl_clean

    # 3) 写回新列
    brand_aliases_new_col = []
    exclusion_field_new_col = []
    for _, row in enriched.iterrows():
        canon = str(row.get("enum_label", ""))
        brand_aliases_new_col.append(join_list_unique(canon_to_alias_new.get(canon, [])))
        exclusion_field_new_col.append(join_list_unique(canon_to_excl_new.get(canon, [])))

    enriched["brand_aliases_new"] = brand_aliases_new_col
    enriched["exclusion_field_new"] = exclusion_field_new_col

    # —— 导出文件
    ensure_dir(OUT_DIR)
    enriched_path = os.path.join(OUT_DIR, "sheet1_enriched.xlsx")
    with pd.ExcelWriter(enriched_path, engine="openpyxl") as w:
        enriched.to_excel(w, index=False, sheet_name="sheet1_enriched")

    # 三清单
    pd.DataFrame(updates_alias_rows).to_csv(os.path.join(OUT_DIR, "updates_alias.csv"), index=False, encoding="utf-8-sig")
    pd.DataFrame(new_brand_rows).to_csv(os.path.join(OUT_DIR, "new_brands.csv"), index=False, encoding="utf-8-sig")
    pd.DataFrame(review_rows).to_csv(os.path.join(OUT_DIR, "review_list.csv"), index=False, encoding="utf-8-sig")

    # —— 简报
    n_hit = int((enriched["decision"] == "ADD_ALIAS").sum())
    n_new = int((enriched["decision"] == "NEW").sum())
    n_review = len(review_rows)
    print("=== 合并完成 ===")
    print(f"强匹配/追加别名: {n_hit}")
    print(f"新增品牌:       {n_new}")
    print(f"待复核:         {n_review}")
    print(f"输出: {enriched_path}")
    print(f"      {os.path.join(OUT_DIR, 'updates_alias.csv')}")
    print(f"      {os.path.join(OUT_DIR, 'new_brands.csv')}")
    print(f"      {os.path.join(OUT_DIR, 'review_list.csv')}")

if __name__ == "__main__":
    main()
```

**说明与使用要点**

* 不会改动你的原始 Excel；所有结果写到 `…/enriched/` 目录。
* 新列 `brand_aliases_new / exclusion_field_new` 是在“合并后”基础上做的**全局扩散**与**反山寨排除**，原列保留便于对比。
* 反山寨规则已覆盖“新中石化/××中石化商贸/…设备”等形态；若你有**官方子公司/合资白名单**，把名单接到脚本顶部的 `OFFICIAL_WHITELIST`（或改为读取 CSV）即可进一步减少误杀。
* 阈值、Top-K、词典都在参数区可调。

需要我把**官方白名单/黑名单**改成读你本地 CSV 的形式，也可以直接给我两份文件名，我帮你补两行 I/O。下一步你跑完看 `review_list.csv` 的样本，咱们把阈值和词典再微调一轮，让“强匹配↑、复核↓”。


---
放哪都能跑，但最省心的是放在**同一目录或其上层**。给你两种稳妥摆放法：

### 方案 A（最简单）

把脚本 `merge_brand_aliases.py` 放到和 Excel 同一层：

```
/Users/didi/Downloads/panth/tag_ct/
├─ 区域策略_品牌映射1.xlsx
└─ merge_brand_aliases.py
```

直接在终端执行：

```bash
cd /Users/didi/Downloads/panth/tag_ct
python3 merge_brand_aliases.py
```

脚本里默认的 `INPUT_XLSX` 已指向该文件路径，跑完会在这里生成 `enriched/` 目录，结果都在里面。

### 方案 B（集中管理代码）

把脚本放到 `panth/` 目录（上层），文件仍在 `tag_ct/` 子目录：

```
/Users/didi/Downloads/panth/
├─ merge_brand_aliases.py
└─ tag_ct/
    └─ 区域策略_品牌映射1.xlsx
```

执行：

```bash
cd /Users/didi/Downloads/panth
python3 merge_brand_aliases.py
```

脚本的 `INPUT_XLSX` 已写成绝对路径，不用改。输出会到：  
`/Users/didi/Downloads/panth/tag_ct/enriched/`

---

### 依赖一次装好

```bash
python3 -m pip install -U pandas openpyxl
```

### 若你改了文件位置或文件名

只需要打开脚本，修改顶部参数区：

```python
INPUT_XLSX = "/绝对路径/到/你的/区域策略_品牌映射1.xlsx"
```

就这些。跑完看 `enriched/` 目录下的：

- `sheet1_enriched.xlsx`
    
- `updates_alias.csv`
    
- `new_brands.csv`
    
- `review_list.csv`
    

需要我把白名单/黑名单改成从 CSV 读，告诉我你放哪、文件名叫什么即可。

---

给你一份“看盘指引”，5 分钟内能把结果读明白、决定怎么落库。

# 文件清单 & 用法

1. **sheet1\_enriched.xlsx**（主结果）

* 含你原表全部列 + 新增辅助列：`s2_hit / s2_enum_label / sim_score / sim_channel / decision`
* 以及两列增强：`brand_aliases_new / exclusion_field_new`
* **怎么看**

  * `decision=ADD_ALIAS`：区域值已与某品牌强匹配，本行是“追加别名”的候选；
  * `decision=NEW`：区域值没匹配到任何品牌，新建了品牌（已给出全字段草案）；
  * 其它为空：没被本次区域值触发（原样保留）。
* **第一轮筛选**（Excel→筛选）：

  * 过滤 `decision=ADD_ALIAS`，抽查 `s2_enum_label` 与 `enum_label/brand_aliases_new` 是否合理；
  * 过滤 `decision=NEW`，抽查自动推断的 `brand_category`、`brand_aliases_new` 是否可用；
* **怎么看两列增强**

  * `brand_aliases_new`：在你原 `brand_aliases` 基础上扩散后的“高信号别名”（去重、去公司尾缀）——作为**候选并集**，不是要你一次性全收录；
  * `exclusion_field_new`：按“反山寨（新××、××商贸/科技等）+ 三桶油/外资互斥 + TopK 近邻冲突”生成的**排除词并集**。用于避免“新中石化”等被误判到大品牌。

2. **updates\_alias.csv**（对既有品牌的“新增别名”清单）

* 列：`enum_code, enum_label, new_alias`。
* **用法**：你审核后，把这些 `new_alias` 合并回主库同一行的 `brand_aliases`（注意去重）。
* **优先级**：先处理它——成本最低、收益最高。

3. **new\_brands.csv**（本轮新增品牌的草案）

* 含一整行全字段（我已按默认规则补齐字段）。
* **用法**：逐条确认是否真的是“独立品牌”，确认后整体 append 回主库；如果只是现有品牌的别名，删除这条并将该别名移到 `updates_alias.csv` 方案。
* **重点看**：`brand_category` 是否合理、`brand_aliases` 是否需要收紧（留高信号短别名，冗长形态不要写进别名）。

4. **review\_list.csv**（待复核对照）

* 列：`s2_enum_label, candidate_enum_label, sim_score, sim_channel`。
* **用法**：这是“分数在【复核区间】”的模糊匹配，不做自动变更。你决定：

  * 归入某品牌 → 记一笔到 `updates_alias.csv`；
  * 不是同一主体 → 记到 `new_brands.csv` 或忽略（看业务价值）。

---

# 推荐的审核顺序（30–60 分钟走完一轮）

1. 打开 **updates\_alias.csv**

   * 规则：**短且常用**的别名优先收录；带地域/公司尾缀/业态后缀（如“××石化加油站”）不建议写入别名，可留给匹配规则层处理。
   * 收录后记一条变更日志（便于回滚）。

2. 打开 **new\_brands.csv**

   * 快速判 3 类：

     * 真的新品牌 → 接收；
     * 合资/多词根（如“中化-道达尔”）→ 独立品牌或列入 review；
     * 山寨/污染（如“新中石化”“××中石化商贸”）→ 不建品牌，改记入**目标大品牌的 `exclusion_field`**。

3. 回到 **sheet1\_enriched.xlsx**

   * 按 `decision=NEW`/`ADD_ALIAS` 过滤浏览，核对增强列：

     * `brand_aliases_new`：从中挑 1–3 个**强信号**补到正式 `brand_aliases`；
     * `exclusion_field_new`：挑能“一眼区分”的冲突词（如“新中石化”“中海油”“SHELL”等对手强词）补到正式 `exclusion_field`。
   * 其余增强词**不必一次性全收**，避免过长。

---

# 准入原则（落库前最后检查）

* **别名（brand\_aliases）**

  * 只收“品牌本名/缩写/英文名”这三类强信号；
  * 不收“地域+品牌”“品牌+业态”“公司尾缀”；
  * 不收容易和他牌混淆的词（这些放到 `exclusion_field`）。
* **排除（exclusion\_field）**

  * 必收：高混淆对手强词（如三桶油之间互相）；
  * 必收：山寨/污染组合（新××、××商贸/科技/设备等）；
  * 不收：过泛词（集团、公司、能源等）。
* **新增品牌（new\_brands）**

  * 若能被官方白名单覆盖（正式合资/子公司）→ 保留；
  * 没证据、像别名/污染 → 别建。

---

# 建议的“合并回主库”动作（稳妥两步走）

1. **先小范围替换**

   * 把你确认过的 `updates_alias.csv` 与 `new_brands.csv` 合并，生成一份“变更版 sheet1”（新文件名，如 `sheet1_final_yyyymmdd.xlsx`）
   * 样本抽查 20 条：搜索这些别名在你历史数据是否被正确命中/不串台。
2. **再换线上底表**

   * 只在下一个批处理周期切换；
   * 保留原版一个周期做回滚保险。

---

# 快速质量自检（3 个数要看）

* `updates_alias.csv` 行数 / `s2_enum_label` 去重数 ≥ **60%**（高说明强匹配好）
* `review_list.csv` / 去重数 ≤ **30%**（低越好）
* `new_brands.csv` 中被你判为“误建”的比例 ≤ **10%**（高了就要调阈值或词典）

---

# 防错清单（常见坑）

* 把“昆仑”误并为独立品牌：**它是中石油的强别名**，不要另起炉灶。
* 把“美孚”当独立零售品牌：**它常是油品品牌**，零售看“埃克森美孚/ExxonMobil”。
* 过度收集别名导致串台：别名越短越强，越容易串台；**用排除词去守边界**。
* “新××”不进别名：统一进对应大品牌的 `exclusion_field`。

---

# 下一步推荐

* 先打开 `updates_alias.csv` 做 10 条抽样确认；
* 如果觉得 `review_list.csv` 太多，告诉我“希望强阈值调到 0.94/0.95”，我给你一版更紧的参数；
* 若你手上有“官方子公司/合资白名单”CSV，给 2–3 条示例字段名，我把脚本改成自动读表合并，复核量会再降一截。



---
收到，我先把“怎么比、怎么判、怎么出”的框架捋清。你回【23】我就给可直接跑的 Python，默认读你这同一路径文件，产出同路径、自动加时间戳。

# 匹配目标

* **输入**：同一 Excel（第2个 sheet、 第3个 sheet），两张表都含 `brand_name`。
* **输出**：逐条把 **sheet2.brand\_name（简称）** 去 **sheet3.brand\_name** 里做匹配，给出最佳匹配与“匹配精准度”。并生成结果表（同目录、带时间戳）。
* **硬规则**：

  1. **中字头严谨 100%**：

     * 中国石化≠中国石油≠中国海油≠中国中化（含各自常用别名/英文缩写）。
     * 一旦 sheet2 是这些家族中的任意形态，只允许匹配到**同家族**（词根一致），否则即使分数高也**判为禁止**（score=0，标注 blocked）。
  2. **“小散”统一 → 其他**：

     * 只要 sheet2 的 `brand_name` **包含“小散”**，直接映射为 `其他`（score=1.0，channel=RULE\_SMALL\_VENDOR），不再对 sheet3 做检索；
     * 若 sheet3 中存在“其他”，则回填为该行；否则结果中 `brand_name_3="其他(虚拟)"` 并标记建议在3中补录。
  3. **强等价示例**：

     * 2=“BP”，3=“中油 BP” → 视为 100% 命中（score=1.0，channel=TOKEN\_CONTAIN），理由：英文强辨识词“BP”**完整词**被包含。
     * 同理：SHELL↔壳牌、TOTAL/TotalEnergies↔道达尔、CNPC↔中国石油/中石油、CNOOC↔中国海油/中海油、SINOPEC↔中国石化/中石化等。

# 评分&通道（从高到低优先）

* **T1 精确等值**（大小写/全半角/公司尾缀规范化后完全一致）= 1.00
* **T2 词根等价**（落在同一家族词根白名单中，如 SINOPEC↔中国石化↔中石化）= 0.99
* **T3 强辨识词包含**（英文品牌缩写作完整词被包含：BP/SHELL/TOTAL/BP/CNPC/CNOOC/SINOPEC/EXXONMOBIL/CALTEX/CHEVRON…）= 0.98
* **T4 子串包含**（长度≥3 的中文/英文词）= 0.95
* **T5 2-gram Jaccard**（中文2-gram/英文按词）∈\[0,1]
* **T6 归一化编辑相似** ∈\[0,1]

> 最终得分取 **max(各通道)**，并记录 `channel` 与 `reason`。
> 设定阈值：`≥0.92` 判为“高置信匹配”；`0.78–0.92` 进入“待复核”；`<0.78` 视为“未匹配”。

# 规范化（仅用于匹配，不改写原文）

* 去空格/标点/全半角统一；
* 去公司尾缀：集团/有限公司/公司/分公司/石油化工/石油/石化/能源/加油站 等；
* 大小写统一；
* 英文词按整词切分，确保 “BP” 不被 “BPP” 误包含；
* 维护 **家族词根表**：

  * 中国石化家族：{中国石化, 中石化, SINOPEC}
  * 中国石油家族：{中国石油, 中石油, CNPC, 昆仑}
  * 中国海油家族：{中国海油, 中海油, 中国海洋石油, CNOOC}
  * 中国中化家族：{中国中化, 中化, SINOCHEM}
  * 外资强词：{壳牌/SHELL, 道达尔/TOTAL/TotalEnergies, BP, 雪佛龙/Chevron, 加德士/Caltex, 埃克森美孚/ExxonMobil/美孚}

# 决策顺序（防误并）

1. 若 **sheet2.brand\_name 含“小散”** → 直接映射 `其他`（不再评分）。
2. 若 **sheet2.brand\_name** 命中任一**中字头家族** → 仅在该家族集合内寻找候选；跨家族命中一律 **blocked**。
3. 运行多通道打分，取最高分做候选；
4. 根据阈值落位：`≥0.92` 命中；`0.78–0.92` 待复核；否则未匹配。
5. 若同时出现多个 `=最高分` 候选，标记 `tie=1`，进入待复核，并给出 Top-N 候选。

# 产出文件（同路径、自动时间戳）

文件名形如：`brand_match_YYYYMMDD_HHMMSS.xlsx`

* **Sheet: match\_result**（逐行结果）

  * `brand_name_2`（原值）
  * `brand_name_3`（命中的 3 的品牌名；未命中留空或“其他(虚拟)”）
  * `score`（0–1）
  * `channel`（T1/T2/T3/T4/T5/T6/RULE\_SMALL\_VENDOR/BLOCKED\_FAMILY）
  * `reason`（如 “BP token contained in ‘中油 BP’” / “family mismatched: SINOPEC vs CNPC”）
  * `status`（HIT/REVIEW/NOMATCH）
  * `strict_family`（命中的家族名或空）
  * `candidate_count`（参与评分的 3 侧候选数）
  * `top3_candidates`（`name:score` 列表，便于肉眼校验）
* **Sheet: review\_only**（`0.78–0.92` & tie 的记录）
* **Sheet: no\_match**（<0.78 的记录）

> 这样你可以先看 `match_result` 的 `status=REVIEW` 和 `BLOCKED_FAMILY`，快速二次确认；`RULE_SMALL_VENDOR` 的行直接当“其他”。

---

如果以上对齐无误，回我 **23**，我立刻给你一份可直接跑的脚本（`pandas + openpyxl`），读取**同一文件**的第 2、3 个 sheet，按上面规则评分输出到同路径（自动时间戳）。

---
好嘞！这是按你“2↔3 按简称→最优匹配”的要求写好的**可直接跑**脚本。它：

* 默认读取：`/Users/didi/Downloads/panth/tag_ct/区域策略_品牌映射1.xlsx`
* 用 sheet **下标**定位：第 **2** 张（index=1）和第 **3** 张（index=2）
* 主键：两张表都用列名 **`brand_name`**
* 严格规则：

  * **“小散”**：sheet2 中凡含“小散”→ 直接映射到“其他”（若 3 中无“其他”，也会标注为“其他(虚拟)”）
  * **中字头 100%**：`中国石化/中国石油/中国海油/中国中化`（含强别名）**只允许**在各自家族内匹配；跨家族一律 `BLOCKED_FAMILY`
* 多通道打分：T1 精确、T2 家族等价、T3 强词包含（如 BP→“中油 BP”=1.0）、T4 子串、T5 Jaccard、T6 编辑相似
* 阈值：`≥0.92` 命中，`[0.78, 0.92)` 复核，`<0.78` 无匹配
* 产出：同路径写入 `brand_match_YYYYMMDD_HHMMSS.xlsx`

  * `match_result`：全量结果（含 `brand_name_2, brand_name_3, score, channel, reason, status, strict_family, candidate_count, top3_candidates`）
  * `review_only`：需要复核
  * `no_match`：未匹配

把下面保存为 `match_sheet2_to_sheet3.py`，在终端跑：

```bash
cd /Users/didi/Downloads/panth/tag_ct
python3 match_sheet2_to_sheet3.py
```

```python
# -*- coding: utf-8 -*-
import os, re, unicodedata, math, datetime
from collections import defaultdict
import pandas as pd

# ========= 参数区 =========
INPUT_XLSX = "/Users/didi/Downloads/panth/tag_ct/区域策略_品牌映射1.xlsx"
SHEET2_IDX = 1  # 第二个 sheet（0-based）
SHEET3_IDX = 2  # 第三个 sheet（0-based）

# 阈值
TH_STRONG = 0.92
TH_REVIEW = 0.78

# “小散”命中即映射“其他”
SMALL_VENDOR_TOKEN = "小散"
OTHER_SYNONYMS_3 = {"其他", "other", "其它"}

# 家族词根（中字头 100% 严格）
FAMILY_ROOTS = {
    "SINOPEC": {"中国石化", "中石化", "sinopec"},
    "CNPC": {"中国石油", "中石油", "cnpc", "昆仑"},
    "CNOOC": {"中国海油", "中海油", "中国海洋石油", "cnooc"},
    "SINOCHEM": {"中国中化", "中化", "sinochem"},
}

# 强辨识英文/中文 Token（用于 T3 强词包含）
STRONG_TOKENS = {
    "BP": {"bp"},
    "SHELL": {"shell", "壳牌"},
    "TOTAL": {"total", "totalenergies", "道达尔"},
    "CHEVRON": {"chevron", "雪佛龙"},
    "CALTEX": {"caltex", "加德士"},
    "EXXONMOBIL": {"exxonmobil", "埃克森美孚", "美孚"},
    "SINOPEC": {"sinopec", "中国石化", "中石化"},
    "CNPC": {"cnpc", "中国石油", "中石油", "昆仑"},
    "CNOOC": {"cnooc", "中国海油", "中海油", "中国海洋石油"},
    "SINOCHEM": {"sinochem", "中国中化", "中化"},
}

# ========= 文本规范化工具 =========
SPACE_PUNC = re.compile(r"[\s·•\-\_（）\(\)【】\[\]<>《》“”\"\'，,。!！?？:：;；/\\]+")

COMPANY_TAIL = re.compile(
    r"(（中国）|中国)?(股份)?有限(责任)?公司|集团|公司|分公司|总公司|石油化工|石油天然气|石油|石化|能源|加油站|有限公司"
)

def z2h(s: str) -> str:
    return unicodedata.normalize("NFKC", s)

def norm(s: str) -> str:
    if not isinstance(s, str): return ""
    s = z2h(s.strip().lower())
    s = SPACE_PUNC.sub("", s)
    s = COMPANY_TAIL.sub("", s)
    return s

def split_words_en(s: str):
    # 英文分词，确保 BP 不被 BPP 误包含
    return re.findall(r"[a-z]+", z2h(s.lower()))

def ngram2(s: str):
    s = norm(s)
    if len(s) <= 2: return {s} if s else set()
    return {s[i:i+2] for i in range(len(s)-2+1)}

def jaccard(a: str, b: str) -> float:
    A, B = ngram2(a), ngram2(b)
    if not A or not B: return 0.0
    return len(A & B) / len(A | B)

def edit_sim(a: str, b: str) -> float:
    a, b = norm(a), norm(b)
    la, lb = len(a), len(b)
    if la == 0 and lb == 0: return 1.0
    if la == 0 or lb == 0: return 0.0
    # Levenshtein
    dp = list(range(lb+1))
    for i in range(1, la+1):
        prev, dp[0] = dp[0], i
        for j in range(1, lb+1):
            cur = dp[j]
            cost = 0 if a[i-1] == b[j-1] else 1
            dp[j] = min(dp[j] + 1, dp[j-1] + 1, prev + cost)
            prev = cur
    dist = dp[lb]
    return 1.0 - dist / max(la, lb)

def token_contains(a: str, b: str) -> bool:
    # 中文/英文：长度>=3 的子串 或 英文整词包含
    an, bn = norm(a), norm(b)
    if len(an) >= 3 and bn in an: return True
    if len(bn) >= 3 and an in bn: return True
    wa, wb = set(split_words_en(a)), set(split_words_en(b))
    if wa and wb and (wa & wb): return True
    return False

# ========= 家族识别 =========
def detect_family(name: str):
    s = z2h(name.lower())
    for fam, roots in FAMILY_ROOTS.items():
        for r in roots:
            if r.lower() in s:
                return fam
    return None

def same_family(a: str, b: str) -> bool:
    fa, fb = detect_family(a), detect_family(b)
    if fa is None or fb is None:
        return True  # 非中字头不强制
    return fa == fb

# ========= 强词包含（T3） =========
def strong_token_hit(a: str, b: str):
    a_words = set(split_words_en(a)) | {z2h(a)}
    b_words = set(split_words_en(b)) | {z2h(b)}
    # 展平强词集
    strong = set()
    for vs in STRONG_TOKENS.values():
        strong |= {z2h(x.lower()) for x in vs}
    # 命中规则：任一强词在 a、b 中以“整词/完整中文词”形式共现或被包含
    for tok in strong:
        if tok in z2h(a.lower()) and tok in z2h(b.lower()):
            return True, tok
        if tok in a_words and tok in b_words:
            return True, tok
    return False, None

# ========= 打分通道 =========
def score_pair(q: str, cand: str):
    qn, cn = norm(q), norm(cand)
    # T1 exact
    if qn and qn == cn:
        return 1.0, "T1", "exact"
    # T2 family equal
    if detect_family(q) and detect_family(cand) and detect_family(q) == detect_family(cand):
        return 0.99, "T2", f"family={detect_family(q)}"
    # T3 strong token contain
    hit, tok = strong_token_hit(q, cand)
    if hit:
        return 0.98, "T3", f"token={tok}"
    # T4 contains
    if token_contains(q, cand):
        return 0.95, "T4", "substring/word overlap"
    # T5 Jaccard
    sc4 = jaccard(q, cand)
    # T6 edit
    sc5 = edit_sim(q, cand)
    if sc4 >= sc5:
        return sc4, "T5", "jaccard-2gram"
    else:
        return sc5, "T6", "edit-sim"

# ========= 主流程 =========
def main():
    xls = pd.ExcelFile(INPUT_XLSX)
    df2 = pd.read_excel(xls, sheet_name=SHEET2_IDX)
    df3 = pd.read_excel(xls, sheet_name=SHEET3_IDX)

    # 列名清洗
    df2.columns = [str(c).strip() for c in df2.columns]
    df3.columns = [str(c).strip() for c in df3.columns]
    if "brand_name" not in df2.columns or "brand_name" not in df3.columns:
        raise ValueError("两张表都必须包含列名 `brand_name`")

    # 3侧“其他”定位（若无，允许虚拟映射）
    others_in_3 = set()
    for v in df3["brand_name"].dropna().astype(str):
        vv = z2h(v.strip()).lower()
        if any(k in vv for k in {k.lower() for k in OTHER_SYNONYMS_3}):
            others_in_3.add(v)

    cand3 = df3["brand_name"].dropna().astype(str).map(str.strip).tolist()

    results = []
    for q in df2["brand_name"].dropna().astype(str).map(str.strip).tolist():
        q_disp = q

        # 规则0：小散→其他
        if SMALL_VENDOR_TOKEN in q:
            if others_in_3:
                # 有“其他”真实存在
                best_name = list(others_in_3)[0]
                results.append({
                    "brand_name_2": q_disp,
                    "brand_name_3": best_name,
                    "score": 1.0,
                    "channel": "RULE_SMALL_VENDOR",
                    "reason": "小散→其他",
                    "status": "HIT",
                    "strict_family": None,
                    "candidate_count": len(cand3),
                    "top3_candidates": ""
                })
            else:
                results.append({
                    "brand_name_2": q_disp,
                    "brand_name_3": "其他(虚拟)",
                    "score": 1.0,
                    "channel": "RULE_SMALL_VENDOR",
                    "reason": "小散→其他(3侧未建)",
                    "status": "HIT",
                    "strict_family": None,
                    "candidate_count": len(cand3),
                    "top3_candidates": ""
                })
            continue

        # 规则1：中字头家族严格
        fam_q = detect_family(q)
        cands = cand3
        family_blocked = False
        if fam_q is not None:
            # 仅在同家族内匹配
            cands_fam = [c for c in cand3 if detect_family(c) == fam_q]
            if not cands_fam:
                # 没有同家族候选 → 直接 BLOCK
                results.append({
                    "brand_name_2": q_disp,
                    "brand_name_3": "",
                    "score": 0.0,
                    "channel": "BLOCKED_FAMILY",
                    "reason": f"family={fam_q} 没有同家族候选",
                    "status": "NOMATCH",
                    "strict_family": fam_q,
                    "candidate_count": len(cand3),
                    "top3_candidates": ""
                })
                continue
            cands = cands_fam

        # 对候选打分
        scored = []
        for c in cands:
            sc, ch, why = score_pair(q, c)
            scored.append((c, sc, ch, why))
        scored.sort(key=lambda x: x[1], reverse=True)

        # Top-1 + Top-3展示
        if not scored:
            results.append({
                "brand_name_2": q_disp,
                "brand_name_3": "",
                "score": 0.0,
                "channel": "NONE",
                "reason": "无候选",
                "status": "NOMATCH",
                "strict_family": fam_q,
                "candidate_count": 0,
                "top3_candidates": ""
            })
            continue

        # 若原本限制了家族，额外防御：最佳候选必须与 q 同家族
        best_name, best_sc, best_ch, best_why = scored[0]
        if fam_q is not None and detect_family(best_name) != fam_q:
            results.append({
                "brand_name_2": q_disp,
                "brand_name_3": "",
                "score": 0.0,
                "channel": "BLOCKED_FAMILY",
                "reason": f"family mismatch: {fam_q} vs {detect_family(best_name)}",
                "status": "NOMATCH",
                "strict_family": fam_q,
                "candidate_count": len(cands),
                "top3_candidates": "; ".join([f"{n}:{round(s,4)}" for n,s,_,__ in scored[:3]])
            })
            continue

        # tie 检查
        tie = False
        if len(scored) > 1 and abs(scored[0][1] - scored[1][1]) < 1e-6:
            tie = True

        status = "HIT" if best_sc >= TH_STRONG and not tie else ("REVIEW" if best_sc >= TH_REVIEW else "NOMATCH")
        reason = best_why
        channel = best_ch
        if tie and status == "HIT":
            status = "REVIEW"
            reason += " (tie)"

        results.append({
            "brand_name_2": q_disp,
            "brand_name_3": best_name if status != "NOMATCH" else "",
            "score": round(float(best_sc), 6),
            "channel": channel,
            "reason": reason,
            "status": status,
            "strict_family": fam_q,
            "candidate_count": len(cands),
            "top3_candidates": "; ".join([f"{n}:{round(s,4)}" for n,s,_,__ in scored[:3]])
        })

    # 输出
    out_dir = os.path.dirname(INPUT_XLSX)
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = os.path.join(out_dir, f"brand_match_{ts}.xlsx")

    # 组装 DataFrame
    df_res = pd.DataFrame(results)
    df_review = df_res[df_res["status"] == "REVIEW"].copy()
    df_nomatch = df_res[df_res["status"] == "NOMATCH"].copy()

    with pd.ExcelWriter(out_path, engine="openpyxl") as w:
        df_res.to_excel(w, index=False, sheet_name="match_result")
        df_review.to_excel(w, index=False, sheet_name="review_only")
        df_nomatch.to_excel(w, index=False, sheet_name="no_match")

    print("=== 匹配完成 ===")
    print(f"输出: {out_path}")
    print(df_res["status"].value_counts())

if __name__ == "__main__":
    main()
```

**小贴士**

* 你的例子“2=BP ↔ 3=中油 BP”会走 **T3 强词包含**，得分直接拉到 **1.0**，`status=HIT`。
* 若你发现“复核太多/太少”，只需把脚本顶部 `TH_STRONG/TH_REVIEW` 微调即可。
* 如果 3 里“其他”写成别的（比如“其它/Other”），我已做了同义识别；若还有别称，往 `OTHER_SYNONYMS_3` 里加。

---
懂了！以「**3 表为主表**」逐条去 2 表里找最优匹配，把 **sheet3.brand\_name** 配上 **sheet2.brand\_name**。下面直接给你一份可跑脚本：读同一路径 Excel 的第 **3** 张表做“参照系”，在结果里写回 `brand_name_3, brand_name_2, score, channel, reason, status…`。规则也同步调整：

* “BP（2）↔ 中油 BP（3）”会命中（强词包含，score=1.0）。
* “中字头”（中石化/中石油/中海油/中化）**只允许同家族**匹配，跨家族直接 `BLOCKED_FAMILY`。
* 2 表里的“**小散**”：只在 3 是“其他/其它/other”时才映射为“其他”，否则从候选里剔除，避免覆盖 3 的大品牌。

把下面保存为 `match_sheet3_ref_sheet2.py`，按原路径运行：

```bash
cd /Users/didi/Downloads/panth/tag_ct
python3 match_sheet3_ref_sheet2.py
```

```python
# -*- coding: utf-8 -*-
import os, re, unicodedata, datetime
import pandas as pd

# ========= 参数区 =========
INPUT_XLSX = "/Users/didi/Downloads/panth/tag_ct/区域策略_品牌映射1.xlsx"
SHEET2_IDX = 1  # 第2张表（0-based）：候选（简称库）
SHEET3_IDX = 2  # 第3张表（0-based）：参照系（主表）

TH_STRONG = 0.92
TH_REVIEW = 0.78

SMALL_VENDOR_TOKEN = "小散"
OTHER_SYNONYMS = {"其他", "其它", "other", "Other", "OTHER"}

FAMILY_ROOTS = {
    "SINOPEC": {"中国石化", "中石化", "sinopec"},
    "CNPC": {"中国石油", "中石油", "cnpc", "昆仑"},
    "CNOOC": {"中国海油", "中海油", "中国海洋石油", "cnooc"},
    "SINOCHEM": {"中国中化", "中化", "sinochem"},
}

STRONG_TOKENS = {
    "BP": {"bp"},
    "SHELL": {"shell", "壳牌"},
    "TOTAL": {"total", "totalenergies", "道达尔"},
    "CHEVRON": {"chevron", "雪佛龙"},
    "CALTEX": {"caltex", "加德士"},
    "EXXONMOBIL": {"exxonmobil", "埃克森美孚", "美孚"},
    "SINOPEC": {"sinopec", "中国石化", "中石化"},
    "CNPC": {"cnpc", "中国石油", "中石油", "昆仑"},
    "CNOOC": {"cnooc", "中国海油", "中海油", "中国海洋石油"},
    "SINOCHEM": {"sinochem", "中国中化", "中化"},
}

SPACE_PUNC = re.compile(r"[\s·•\-\_（）\(\)【】\[\]<>《》“”\"\'，,。!！?？:：;；/\\]+")
COMPANY_TAIL = re.compile(
    r"(（中国）|中国)?(股份)?有限(责任)?公司|集团|公司|分公司|总公司|石油化工|石油天然气|石油|石化|能源|加油站|有限公司"
)

def z2h(s: str) -> str:
    return unicodedata.normalize("NFKC", s)

def norm(s: str) -> str:
    if not isinstance(s, str): return ""
    s = z2h(s.strip().lower())
    s = SPACE_PUNC.sub("", s)
    s = COMPANY_TAIL.sub("", s)
    return s

def split_words_en(s: str):
    return re.findall(r"[a-z]+", z2h(s.lower()))

def ngram2(s: str):
    s = norm(s)
    if len(s) <= 2: return {s} if s else set()
    return {s[i:i+2] for i in range(len(s)-1)}

def jaccard(a: str, b: str) -> float:
    A, B = ngram2(a), ngram2(b)
    if not A or not B: return 0.0
    return len(A & B) / len(A | B)

def edit_sim(a: str, b: str) -> float:
    a, b = norm(a), norm(b)
    la, lb = len(a), len(b)
    if la == 0 and lb == 0: return 1.0
    if la == 0 or lb == 0: return 0.0
    dp = list(range(lb+1))
    for i in range(1, la+1):
        prev, dp[0] = dp[0], i
        for j in range(1, lb+1):
            cur = dp[j]
            cost = 0 if a[i-1] == b[j-1] else 1
            dp[j] = min(dp[j] + 1, dp[j-1] + 1, prev + cost)
            prev = cur
    dist = dp[lb]
    return 1.0 - dist / max(la, lb)

def token_contains(a: str, b: str) -> bool:
    an, bn = norm(a), norm(b)
    if len(an) >= 3 and bn in an: return True
    if len(bn) >= 3 and an in bn: return True
    wa, wb = set(split_words_en(a)), set(split_words_en(b))
    if wa and wb and (wa & wb): return True
    return False

def detect_family(name: str):
    s = z2h(name.lower())
    for fam, roots in FAMILY_ROOTS.items():
        for r in roots:
            if r.lower() in s:
                return fam
    return None

def strong_token_hit(a: str, b: str):
    a_words = set(split_words_en(a)) | {z2h(a)}
    b_words = set(split_words_en(b)) | {z2h(b)}
    strong = set()
    for vs in STRONG_TOKENS.values():
        strong |= {z2h(x.lower()) for x in vs}
    for tok in strong:
        if tok in z2h(a.lower()) and tok in z2h(b.lower()):
            return True, tok
        if tok in a_words and tok in b_words:
            return True, tok
    return False, None

def score_pair(q3: str, c2: str):
    qn, cn = norm(q3), norm(c2)
    if qn and qn == cn:
        return 1.0, "T1", "exact"
    fq, fc = detect_family(q3), detect_family(c2)
    if fq and fc and fq == fc:
        return 0.99, "T2", f"family={fq}"
    hit, tok = strong_token_hit(q3, c2)
    if hit:
        return 0.98, "T3", f"token={tok}"
    if token_contains(q3, c2):
        return 0.95, "T4", "substring/word overlap"
    sc4 = jaccard(q3, c2)
    sc5 = edit_sim(q3, c2)
    if sc4 >= sc5:
        return sc4, "T5", "jaccard-2gram"
    else:
        return sc5, "T6", "edit-sim"

def main():
    xls = pd.ExcelFile(INPUT_XLSX)
    df2 = pd.read_excel(xls, sheet_name=SHEET2_IDX)
    df3 = pd.read_excel(xls, sheet_name=SHEET3_IDX)

    # 列名
    df2.columns = [str(c).strip() for c in df2.columns]
    df3.columns = [str(c).strip() for c in df3.columns]
    if "brand_name" not in df2.columns or "brand_name" not in df3.columns:
        raise ValueError("两张表都必须包含列名 `brand_name`")

    # 2表候选清单
    cand2_all = df2["brand_name"].dropna().astype(str).map(str.strip).tolist()

    # 3表“其他”识别
    def is_other(s: str) -> bool:
        return z2h(str(s)).strip() in OTHER_SYNONYMS or any(k in z2h(str(s)).lower() for k in {k.lower() for k in OTHER_SYNONYMS})

    results = []
    for q3 in df3["brand_name"].dropna().astype(str).map(str.strip).tolist():
        fam_q = detect_family(q3)
        q3_is_other = is_other(q3)

        # 候选集：默认取 2 的全部；但若 q3 是中字头 → 只取同家族候选
        cands = cand2_all
        if fam_q:
            cands = [c for c in cand2_all if detect_family(c) == fam_q]
            if not cands:
                results.append({
                    "brand_name_3": q3,
                    "brand_name_2": "",
                    "score": 0.0,
                    "channel": "BLOCKED_FAMILY",
                    "reason": f"family={fam_q} 在2表无同家族候选",
                    "status": "NOMATCH",
                    "strict_family": fam_q,
                    "candidate_count": len(cand2_all),
                    "top3_candidates": ""
                })
                continue
        # 小散处理：只有当 q3 是“其他”时，才允许小散作为候选（映射为其他）
        if not q3_is_other:
            cands = [c for c in cands if SMALL_VENDOR_TOKEN not in c]

        # 如果 q3 是“其他”：强制命中“小散”或 2里“其他/other”
        if q3_is_other:
            # 优先找2的“其他/小散”
            c_other = None
            for c in cand2_all:
                if SMALL_VENDOR_TOKEN in c or is_other(c):
                    c_other = c
                    break
            if c_other:
                results.append({
                    "brand_name_3": q3,
                    "brand_name_2": "其他",
                    "score": 1.0,
                    "channel": "RULE_SMALL_VENDOR",
                    "reason": "3=其他，2含小散/其他 → 映射其他",
                    "status": "HIT",
                    "strict_family": None,
                    "candidate_count": len(cand2_all),
                    "top3_candidates": ""
                })
                continue
            # 没有小散也继续常规匹配（可能 2 有“其他”的别称）
            # 不 return，继续走打分

        # 候选为空
        if not cands:
            results.append({
                "brand_name_3": q3,
                "brand_name_2": "",
                "score": 0.0,
                "channel": "NONE",
                "reason": "无候选（小散被剔除或家族过滤后为空）",
                "status": "NOMATCH",
                "strict_family": fam_q,
                "candidate_count": 0,
                "top3_candidates": ""
            })
            continue

        # 打分
        scored = []
        for c in cands:
            sc, ch, why = score_pair(q3, c)
            scored.append((c, sc, ch, why))
        scored.sort(key=lambda x: x[1], reverse=True)

        best_name, best_sc, best_ch, best_why = scored[0]
        # 若 q3 是中字头，再次严格校验最佳候选必须同家族
        if fam_q and detect_family(best_name) != fam_q:
            results.append({
                "brand_name_3": q3,
                "brand_name_2": "",
                "score": 0.0,
                "channel": "BLOCKED_FAMILY",
                "reason": f"family mismatch: {fam_q} vs {detect_family(best_name)}",
                "status": "NOMATCH",
                "strict_family": fam_q,
                "candidate_count": len(cands),
                "top3_candidates": "; ".join([f"{n}:{round(s,4)}" for n,s,_,__ in scored[:3]])
            })
            continue

        # tie 检查
        tie = False
        if len(scored) > 1 and abs(scored[0][1] - scored[1][1]) < 1e-6:
            tie = True

        status = "HIT" if best_sc >= TH_STRONG and not tie else ("REVIEW" if best_sc >= TH_REVIEW else "NOMATCH")
        reason = best_why
        if tie and status == "HIT":
            status = "REVIEW"
            reason += " (tie)"

        results.append({
            "brand_name_3": q3,
            "brand_name_2": best_name if status != "NOMATCH" else "",
            "score": round(float(best_sc), 6),
            "channel": best_ch,
            "reason": reason,
            "status": status,
            "strict_family": fam_q,
            "candidate_count": len(cands),
            "top3_candidates": "; ".join([f"{n}:{round(s,4)}" for n,s,_,__ in scored[:3]])
        })

    # 输出
    out_dir = os.path.dirname(INPUT_XLSX)
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = os.path.join(out_dir, f"brand_match_ref3_{ts}.xlsx")

    df_res = pd.DataFrame(results)
    df_review = df_res[df_res["status"] == "REVIEW"].copy()
    df_nomatch = df_res[df_res["status"] == "NOMATCH"].copy()

    with pd.ExcelWriter(out_path, engine="openpyxl") as w:
        df_res.to_excel(w, index=False, sheet_name="match_result")
        df_review.to_excel(w, index=False, sheet_name="review_only")
        df_nomatch.to_excel(w, index=False, sheet_name="no_match")

    print("=== 匹配完成（3 为参照系） ===")
    print(f"输出: {out_path}")
    print(df_res["status"].value_counts())

if __name__ == "__main__":
    main()
```

**怎么用**

* 把脚本放在同目录或上层目录运行即可；依赖 `pandas openpyxl`。
* 产物：`brand_match_ref3_YYYYMMDD_HHMMSS.xlsx`（同路径）。
* 读法跟你前一版一样看 `status` 列：`HIT/REVIEW/NOMATCH`，优先审 `REVIEW`。
* 若希望更“谨慎”，把 `TH_STRONG=0.94`；若想更“激进”，把 `TH_REVIEW=0.75`。

---
明白了：**3 表=原始值**，**2 表=我之前“归化后的标准名”**。目标：用 3 的原始 `brand_name` 去映射到 2 的标准 `brand_name`。规则补强两点：

* 3 含有“**小散**”→ 一律映射到 **其他**（若 2 没有“其他”，就标注为“其他(虚拟)”）。
* 3 的“地域-品牌”之类（如“**大连-中石化**”）自动剥掉地域前缀，按品牌段做匹配；中字头必须同家族匹配，避免“中石化↔中石油”串台。

把下面保存为 `match_sheet3_ref_sheet2_v2.py`，放在 `/Users/didi/Downloads/panth/tag_ct` 跑即可：

```bash
cd /Users/didi/Downloads/panth/tag_ct
python3 match_sheet3_ref_sheet2_v2.py
```

```python
# -*- coding: utf-8 -*-
import os, re, unicodedata, datetime
import pandas as pd

# ========= 参数区 =========
INPUT_XLSX = "/Users/didi/Downloads/panth/tag_ct/区域策略_品牌映射1.xlsx"
SHEET2_IDX = 1  # 第2张表（0-based）：标准/归化后的brand_name
SHEET3_IDX = 2  # 第3张表（0-based）：原始brand_name（含地域前缀/杂词等）

TH_STRONG = 0.92
TH_REVIEW = 0.78

SMALL_VENDOR_TOKEN = "小散"
OTHER_SYNONYMS = {"其他", "其它", "other", "Other", "OTHER"}

# 中字头家族（同家族才允许匹配）
FAMILY_ROOTS = {
    "SINOPEC": {"中国石化", "中石化", "sinopec"},
    "CNPC": {"中国石油", "中石油", "cnpc", "昆仑"},
    "CNOOC": {"中国海油", "中海油", "中国海洋石油", "cnooc"},
    "SINOCHEM": {"中国中化", "中化", "sinochem"},
}

# 强辨识 token（英文/中文）
STRONG_TOKENS = {
    "BP": {"bp"},
    "SHELL": {"shell", "壳牌"},
    "TOTAL": {"total", "totalenergies", "道达尔"},
    "CHEVRON": {"chevron", "雪佛龙"},
    "CALTEX": {"caltex", "加德士"},
    "EXXONMOBIL": {"exxonmobil", "埃克森美孚", "美孚"},
    "SINOPEC": {"sinopec", "中国石化", "中石化"},
    "CNPC": {"cnpc", "中国石油", "中石油", "昆仑"},
    "CNOOC": {"cnooc", "中国海油", "中海油", "中国海洋石油"},
    "SINOCHEM": {"sinochem", "中国中化", "中化"},
}

# 规范化工具
SPACE_PUNC = re.compile(r"[\s·•\-\_（）\(\)【】\[\]<>《》“”\"\'，,。!！?？:：;；/\\]+")
COMPANY_TAIL = re.compile(
    r"(（中国）|中国)?(股份)?有限(责任)?公司|集团|公司|分公司|总公司|石油化工|石油天然气|石油|石化|能源|加油站|有限公司"
)
REGION_SPLITTER = re.compile(r"[ \-—_·|]+")  # 用于剥地域前缀：如"大连-中石化"

def z2h(s: str) -> str:
    return unicodedata.normalize("NFKC", s)

def norm(s: str) -> str:
    if not isinstance(s, str): return ""
    s = z2h(s.strip().lower())
    s = SPACE_PUNC.sub("", s)
    s = COMPANY_TAIL.sub("", s)
    return s

def split_words_en(s: str):
    return re.findall(r"[a-z]+", z2h(s.lower()))

def ngram2(s: str):
    s = norm(s)
    if len(s) <= 2: return {s} if s else set()
    return {s[i:i+2] for i in range(len(s)-1)}

def jaccard(a: str, b: str) -> float:
    A, B = ngram2(a), ngram2(b)
    if not A or not B: return 0.0
    return len(A & B) / len(A | B)

def edit_sim(a: str, b: str) -> float:
    a, b = norm(a), norm(b)
    la, lb = len(a), len(b)
    if la == 0 and lb == 0: return 1.0
    if la == 0 or lb == 0: return 0.0
    dp = list(range(lb+1))
    for i in range(1, la+1):
        prev, dp[0] = dp[0], i
        for j in range(1, lb+1):
            cur = dp[j]
            cost = 0 if a[i-1] == b[j-1] else 1
            dp[j] = min(dp[j] + 1, dp[j-1] + 1, prev + cost)
            prev = cur
    dist = dp[lb]
    return 1.0 - dist / max(la, lb)

def token_contains(a: str, b: str) -> bool:
    an, bn = norm(a), norm(b)
    if len(an) >= 3 and bn in an: return True
    if len(bn) >= 3 and an in bn: return True
    wa, wb = set(split_words_en(a)), set(split_words_en(b))
    if wa and wb and (wa & wb): return True
    return False

def detect_family(name: str):
    s = z2h(name.lower())
    for fam, roots in FAMILY_ROOTS.items():
        for r in roots:
            if r.lower() in s:
                return fam
    return None

def strong_token_hit(a: str, b: str):
    a_words = set(split_words_en(a)) | {z2h(a)}
    b_words = set(split_words_en(b)) | {z2h(b)}
    strong = set()
    for vs in STRONG_TOKENS.values():
        strong |= {z2h(x.lower()) for x in vs}
    for tok in strong:
        if tok in z2h(a.lower()) and tok in z2h(b.lower()):
            return True, tok
        if tok in a_words and tok in b_words:
            return True, tok
    return False, None

# 从原始串里抽“品牌段”（剥掉地域/分隔符，优先选择包含家族/强词的片段）
def extract_brand_segment(s: str) -> str:
    parts = [p for p in REGION_SPLITTER.split(z2h(s)) if p.strip()]
    if not parts: return s
    # 优先返回包含强家族/强token的片段
    for p in parts[::-1]:  # 通常品牌在右侧，逆序更稳
        if detect_family(p): return p
        hit, _ = strong_token_hit(p, p)
        if hit: return p
    # 其次返回最长片段
    return max(parts, key=lambda x: len(x))

def score_pair(q3_raw: str, q3_clean: str, c2: str):
    # q3_clean 用于匹配，q3_raw仅用于输出
    qn, cn = norm(q3_clean), norm(c2)
    if qn and qn == cn:
        return 1.0, "T1", "exact"
    fq, fc = detect_family(q3_clean), detect_family(c2)
    if fq and fc and fq == fc:
        return 0.99, "T2", f"family={fq}"
    hit, tok = strong_token_hit(q3_clean, c2)
    if hit:
        return 0.98, "T3", f"token={tok}"
    if token_contains(q3_clean, c2):
        return 0.95, "T4", "substring/word overlap"
    sc4 = jaccard(q3_clean, c2)
    sc5 = edit_sim(q3_clean, c2)
    if sc4 >= sc5:
        return sc4, "T5", "jaccard-2gram"
    else:
        return sc5, "T6", "edit-sim"

def main():
    xls = pd.ExcelFile(INPUT_XLSX)
    df2 = pd.read_excel(xls, sheet_name=SHEET2_IDX)
    df3 = pd.read_excel(xls, sheet_name=SHEET3_IDX)

    # 列名
    df2.columns = [str(c).strip() for c in df2.columns]
    df3.columns = [str(c).strip() for c in df3.columns]
    if "brand_name" not in df2.columns or "brand_name" not in df3.columns:
        raise ValueError("两张表都必须包含列名 `brand_name`")

    # 2表标准名（候选）
    cand2_all = df2["brand_name"].dropna().astype(str).map(str.strip).tolist()
    cand2_set_lower = {z2h(c).strip().lower() for c in cand2_all}
    has_other_in_2 = any(z2h(c).strip() in OTHER_SYNONYMS or z2h(c).strip().lower() in {k.lower() for k in OTHER_SYNONYMS} for c in cand2_all)

    def std_other_name():
        # 统一回填“其他”
        for c in cand2_all:
            cc = z2h(c).strip()
            if cc in OTHER_SYNONYMS or cc.lower() in {k.lower() for k in OTHER_SYNONYMS}:
                return c
        return "其他(虚拟)"

    results = []
    for q3 in df3["brand_name"].dropna().astype(str).map(str.strip).tolist():
        q3_clean = extract_brand_segment(q3)

        # 规则A：原始值包含“小散” → 映射“其他”
        if SMALL_VENDOR_TOKEN in q3:
            results.append({
                "brand_name_3_raw": q3,
                "brand_name_3_clean": q3_clean,
                "brand_name_2_std": std_other_name(),
                "score": 1.0,
                "channel": "RULE_SMALL_VENDOR",
                "reason": "3含小散→归化为其他",
                "status": "HIT",
                "strict_family": None,
                "candidate_count": len(cand2_all),
                "top3_candidates": ""
            })
            continue

        # 家族限制：若q3_clean识别到中字头，则只在同家族候选里匹配
        fam_q = detect_family(q3_clean)
        cands = cand2_all
        if fam_q:
            cands = [c for c in cand2_all if detect_family(c) == fam_q]
            if not cands:
                results.append({
                    "brand_name_3_raw": q3,
                    "brand_name_3_clean": q3_clean,
                    "brand_name_2_std": "",
                    "score": 0.0,
                    "channel": "BLOCKED_FAMILY",
                    "reason": f"family={fam_q} 在2表无同家族候选",
                    "status": "NOMATCH",
                    "strict_family": fam_q,
                    "candidate_count": len(cand2_all),
                    "top3_candidates": ""
                })
                continue

        # 打分
        if not cands:
            results.append({
                "brand_name_3_raw": q3,
                "brand_name_3_clean": q3_clean,
                "brand_name_2_std": "",
                "score": 0.0,
                "channel": "NONE",
                "reason": "无候选",
                "status": "NOMATCH",
                "strict_family": fam_q,
                "candidate_count": 0,
                "top3_candidates": ""
            })
            continue

        scored = []
        for c in cands:
            sc, ch, why = score_pair(q3, q3_clean, c)
            scored.append((c, sc, ch, why))
        scored.sort(key=lambda x: x[1], reverse=True)

        best_name, best_sc, best_ch, best_why = scored[0]

        # 若识别到中字头，再强校验一次
        if fam_q and detect_family(best_name) != fam_q:
            results.append({
                "brand_name_3_raw": q3,
                "brand_name_3_clean": q3_clean,
                "brand_name_2_std": "",
                "score": 0.0,
                "channel": "BLOCKED_FAMILY",
                "reason": f"family mismatch: {fam_q} vs {detect_family(best_name)}",
                "status": "NOMATCH",
                "strict_family": fam_q,
                "candidate_count": len(cands),
                "top3_candidates": "; ".join([f"{n}:{round(s,4)}" for n,s,_,__ in scored[:3]])
            })
            continue

        # tie → REVIEW
        tie = len(scored) > 1 and abs(scored[0][1] - scored[1][1]) < 1e-6
        status = "HIT" if best_sc >= TH_STRONG and not tie else ("REVIEW" if best_sc >= TH_REVIEW else "NOMATCH")
        reason = best_why + (" (tie)" if tie and status == "REVIEW" else "")

        results.append({
            "brand_name_3_raw": q3,
            "brand_name_3_clean": q3_clean,
            "brand_name_2_std": best_name if status != "NOMATCH" else "",
            "score": round(float(best_sc), 6),
            "channel": best_ch,
            "reason": reason,
            "status": status,
            "strict_family": fam_q,
            "candidate_count": len(cands),
            "top3_candidates": "; ".join([f"{n}:{round(s,4)}" for n,s,_,__ in scored[:3]])
        })

    # 输出
    out_dir = os.path.dirname(INPUT_XLSX)
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = os.path.join(out_dir, f"brand_match_ref3_v2_{ts}.xlsx")

    df_res = pd.DataFrame(results)
    df_review = df_res[df_res["status"] == "REVIEW"].copy()
    df_nomatch = df_res[df_res["status"] == "NOMATCH"].copy()

    with pd.ExcelWriter(out_path, engine="openpyxl") as w:
        df_res.to_excel(w, index=False, sheet_name="match_result")
        df_review.to_excel(w, index=False, sheet_name="review_only")
        df_nomatch.to_excel(w, index=False, sheet_name="no_match")

    print("=== 匹配完成（3原始→2标准；含小散→其他；剥地域前缀） ===")
    print(f"输出: {out_path}")
    print(df_res["status"].value_counts())

if __name__ == "__main__":
    main()
```

**怎么读结果**

* `brand_name_3_raw`：3表原值（如“大连-中石化”）
* `brand_name_3_clean`：自动抽出的“品牌段”（如“中石化”）
* `brand_name_2_std`：2表命中的标准名（如“中国石化”）
* `status`：`HIT/REVIEW/NOMATCH`（先看 REVIEW）
* `channel/reason`：为什么命中，比如 `T2 family=SINOPEC`、`RULE_SMALL_VENDOR` 等

如果你想把“地域列表”也当黑名单再剥更狠（比如识别全国省市县名），我可以再加一个城市词库 CSV 读取，进一步提升 `brand_name_3_clean` 的抽取准确度。
