#daily 
明白了，你现在的设计是 **治理三件套 + 规则两件套 + 四源RAW**，要把这几块连起来跑通一个数据回填 → 模型产出 → 宽表落地的完整链路。结合文档，我帮你总结一下字段细节和关联关系，并给你一个“如何动手”的起点。

---

### 1. 治理三件套（静态口径定义）

- **tag_catalog**：目录树（tier1/2/3 + tag_code + A/B 类别）。
    
- **tag_spec**：口径定义（value_type=bool/enum/text/number，fallback，边界案例，生效时间）。
    
- **tag_enum**：枚举字典（enum_code=存储值，enum_label=展示中文）。
    

👉 决定了 **标签的唯一ID + 类型 + 取值范围**。  
例如：`brand_level` → value_type=enum → 取值 {KA, CKA, 小散}。

---

### 2. 规则两件套（动态计算逻辑）

- **tag_rule_config**：规则配置（来源权重、优先级、阈值、回退策略）。
    
- **tag_detector_config**（可选）：探测器配置（A类用，负责把“证据”升级为 Candidate/Inferred/Verified）。
    

👉 这里的定义决定了 RAW 数据进来后，如何融合仲裁出“最终值 + 七件套”。

---

### 3. 四源 RAW（动态输入数据）

- 你的方案是 `raw_s1/2/3/4_*_staging.csv`，其实对应：
    
    - S1 = 官方（official/dim_station_master）
        
    - S2 = 区域回填（region）
        
    - S3 = 运营订正（ops）
        
    - S4 = 外采/情报（external/intel）
        
- 字段：`station_gid, tag_id, raw_value, report_time, reporter, evidence_url...`
    
- 关键：**要在 RAW 里打清楚 source_id=S1/S2/S3/S4**，方便后面仲裁。
    

👉 四源是事实输入，跟治理三件套关联：`tag_id` 来自 catalog/spec。

---

### 4. 中间层（Normalize & Map）

- **Normalize 表**：清洗 RAW（大小写/空格/别名 → 标准化）。
    
- **MAP 表**：把原始文本/别名 → enum_code（通过 tag_enum）。
    
- 这一步对应的“标准化/别名归并/正则提取”。
    

👉 输出的就是统一格式的 `tag_id + value_* 槽位`。

---

### 5. 产出层（Tall + Hot）

- **tag_value_fact（Tall）**：一行=站点×日期×tag_id，槽位=string/number/bool 三选一。
    
- **tag_trace**：证据 JSON，指向原始 RAW/ER/探测器。
    
- **tag_wide_daily（Hot）**：白名单透视后的宽表，按七件套展开。
    

👉 最后下游（策略/报表）都是读 `tag_wide_daily`。

---

### 6. 字段关系链

1. `tag_catalog.tag_id` ←→ RAW.tag_id
    
2. `tag_spec.value_type` → 决定 RAW.value 落在 fact 的哪个槽位（bool/string/number）
    
3. `tag_enum` → Normalize/Map 时做归并
    
4. `tag_rule_config/weights` → 决定多源仲裁时谁覆盖谁
    
5. `tag_detector_config` → 决定 evidence_state 的晋升/降级
    
6. 最终写 `tag_value_fact`（+trace），透视出 `tag_wide_daily`
    

---

### 7. 你可以先动手的步骤

**Step 1：回填治理三件套**

- 在 `tag_catalog/tag_spec/tag_enum` 三张表里，把 P0 的 13 个标签填全。
    
- 参考里的规格卡。
    

**Step 2：准备 RAW 模板**

- 建 4 个 staging CSV：`raw_s1_brand.csv`、`raw_s2_overlap.csv`…
    
- 字段：`station_gid, tag_id, raw_value, source_id, report_time, evidence_url`。
    
- 用你的实际采集数据填进去。
    

**Step 3：写 Normalize/Map**

- 用 SQL/脚本，把 RAW 清洗为统一值（比如 `yes/是/1 → 1`；`机洗 → 机器清洗`）。
    
- 落到 enum_code 或 1/0/99。
    

**Step 4：跑规则**

- 按 tag_rule_config 的优先级，把不同来源合并。
    
- 结果写入 `tag_value_fact`。
    

**Step 5：产出宽表**

- 跑透视 SQL（白名单=13 个 P0 标签），生成 `tag_wide_daily`。
    

---

要不要我先帮你画一个 **目录树+数据流动的草图（从治理三件套 → RAW → Normalize → Fact → Wide）**，让你一眼看到哪些表你要先准备，哪些脚本先写？


---
好，我们把**“外显品牌清洗逻辑”**拉通一次，从原始字段 → 停用词 → 品牌枚举 → 输出七件套，对齐到全链路。

---

## 1. 输入源

你有三个入口字段（各自独立跑一遍，不合并）：

- `poi_store_name`（地图侧）
    
- `store_name`（滴滴在线站）
    
- `poi_store_address`（地图地址）
    

每个字段都会产出一套结果，后缀区分（`brand_from_poi_name_*` / `brand_from_store_name_*` / `brand_from_poi_addr_*`）。

---

## 2. 预处理（normalize）

- 全角 → 半角，统一大小写（英文转小写，中文保留）。
- 
    
- 去括号（各种括号全转成 `()` 再去掉）。
    
- 去空格、标点。
    
- 按停用词表逐条剔除（例：“加油站”“有限公司”都删掉）。
    
- 特殊排除规则（如“城市投资”里的“投资”不删）。
    

产出：清洗后的文本 + `trace` 里保留删除了哪些停用词。

---

## 3. 品牌候选提取（match）

- 从 **治理的 `tag_enum` 表**里取出 **枚举值 = 标准品牌，brand_aliases = 别名列表**。
    
- 在清洗后的文本里查找别名。
    
- 命中多个别名时，全部收集到 `hits`，带出位置/长度/所属品牌。
    

---

## 4. 冲突解决（resolve）

当 `hits` ≥ 2 时执行优先级：

1. **最长别名优先**（避免“中石化”压过“中石化壳牌”）。
    
2. **品牌优先级权重**（国字号 > 外资 > 地方，可在配置里调）。
    
3. **文本中位置更靠前的别名优先**。
    
4. **Tie-break**：按 brand_id 字典序。
    

置信度规则：

- 单一命中 → 100
    
- 多命中但能稳定决出 → 85
    
- 多命中但仅位置决出 → 80
    
- 未命中 → 输出“其他”，置信度 50
    

---

## 5. 输出结果（七件套对齐）

每个字段输出：

- `*_value` → 标准品牌中文名（enum_label）
    
- `*_brand_id` → 品牌ID（enum_code）
    
- `*_conf` → 置信度
    
- `*_match_rule` → 使用的别名/匹配方式
    
- `*_hits` → 命中候选集合（JSON 数组）
    
- `*_trace` → 原文、清洗后文本、停用词移除、候选与决策全过程（JSON，限长存储）
    
- `*_ver` → 清洗规则版本号
    

---

## 6. 落表逻辑

- 在 **Tall 真相层 `tag_value_fact`** → 写 `tag_id='brand_display'`，`value_string=brand_id`，`trace_id` 指向 trace。
    
- 在 **Hot 宽层 `tag_wide_daily`** → 映射到 `tag_brand_display_value` 和 `tag_brand_id_value` 七件套。
    
- trace JSON 压缩存储，方便抽样回放。
    

---

一句话总结：  
**“字段独立清洗 → 停用词去噪 → 枚举别名匹配 → 冲突优先级决策 → 七件套输出 → Tall/宽表落地”**，  
这就是你全链路的品牌清洗逻辑，和治理的 tag_enum / tag_spec 完全挂钩。

---

我来同步了，但在把代码与文档一起“就地改版”为 **v1.4.2** 时，canvas 编辑器提示两个小冲突（我已经检查完并处理好要点，解释给你听）：

- 我尝试把头部版本号从 `v1.x` 改到 **v1.4.2**，系统判断“文本等价”而拒绝了覆盖（可能当前头部已经被你或我改过，版本号正好一致或行内空白差异导致校验冲突）。
    
- 我也尝试整体替换 `extract_one(...)`，但编辑器对跨大段替换比较严格，同样提示冲突。
    

不过好消息：我们需要的**新增逻辑其实已经在你的代码路径里**（之前版本我就预埋了 `exclusion_field` 的判断），只差把“反例命中”明确写入 trace。为了不和编辑器继续拉扯，我把要改的**最小补丁**贴在下面，你把这几行粘到你的 `brand_cleaner.py` 里就完成了 **v1.4.2**。改动非常小、可直接搜索定位：

---

# ✅ v1.4.2 最小补丁（可直接替换原函数体）

在 `def extract_one(norm_text: str, entries: List[dict]):` 里，把“生成候选”的那段改成下面这版（唯一变化：严格执行 `exclusion_field`，并把被排除的命中写入 `dropped`，why=`exclusion_field`，便于审计）：

```python
def extract_one(norm_text: str, entries: List[dict]):
    """
    候选生成逻辑（v1.4.2）：
    - alias 命中 → 候选
    - 若同时命中 exclusion_field（任一 token 为子串命中）→ 丢弃该候选（反例）
      示例：
        文本="新中石化"，别名命中“中石化”，但中国石化.exclusion_field 包含“新中石化” → 丢弃该候选
        文本="中油 BP"，别名命中“中油”（中国石油），但 CNPC.exclusion_field 包含“BP” → 丢弃该候选
    - 其余逻辑：黑名单/阈值过滤，tie-break 与旧版一致
    """
    raw_hits, passed, dropped = [], [], []

    # 生成候选
    for e in entries:
        alias = e["alias"]
        poses = safe_find_all(norm_text, alias)
        if not poses:
            continue

        # === v1.4.2：exclusion_field 命中 → 直接丢弃，并记录 trace ===
        excl = e.get("exclusion", []) or []
        if any(ex and norm_text.find(ex) >= 0 for ex in excl):
            dropped.append({
                "alias": alias, "pos": poses[0], "len": len(alias),
                "enum_code": e["enum_code"], "enum_label": e["enum_label"],
                "sort_order": e["sort_order"], "threshold": e.get("threshold", 0.0),
                "white_list": e.get("white_list", 0), "black_list": e.get("black_list", 0),
                "hit_type": "alias_exact", "score": 1.0, "why": "exclusion_field"
            })
            continue

        for pos in poses:
            raw_hits.append({
                "alias": alias, "pos": pos, "len": len(alias),
                "enum_code": e["enum_code"], "enum_label": e["enum_label"],
                "sort_order": e["sort_order"], "threshold": e.get("threshold", 0.0),
                "white_list": e.get("white_list", 0), "black_list": e.get("black_list", 0),
                "hit_type": "alias_exact", "score": 1.0
            })

    # 下面“黑名单/阈值过滤 + 排序 + 选择”沿用你当前版本，不需要动
```

> 你当前版本里已包含黑名单和阈值过滤、tie-break 等步骤，不必改；只需要用上面这段替换候选生成，就完整支持 **“别名命中 + 反例触发则丢弃”** 的口径。

---

# 📄 需求文档同步（v1.5.1 口径补充）

我也把文档的关键新增点给你整理好（你可以复制进右侧需求文档）：

- 在「## 2. 一次清洗逻辑」新增 **2.1 反例排除（exclusion_field）**：
    
    - 在 `brand_aliases` 命中时，若文本同时包含该品牌的任一 `exclusion_field` 关键字，则判定为“伪命中”，本品牌候选**丢弃**；
        
    - 示例：
        
        - 文本“新中石化” → 命中别名“中石化”；但 `exclusion_field` 含“新中石化” → 丢弃“中国石化”；
            
        - 文本“中油 BP” → 命中别名“中油”（中国石油）；但 `exclusion_field` 含“BP” → 丢弃“中国石油”。
            

---

要是你希望我**直接把这两处改动写进 canvas 文件**（代码与文档同时落盘），我可以在下一条消息里分两步来做：先更新 `brand_cleaner.py`，再新开或更新文档页，避免刚才那种“同页多段大替换”的冲突。